{
    "project_name": "Angel-ML-angel",
    "violation_id": "693",
    "information": {
        "violations": [
            {
                "line": "6",
                "severity": "error",
                "message": "Line is longer than 100 characters (found 102).",
                "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
            }
        ]
    },
    "source_code": " * Copyright (C) 2017-2018 THL A29 Limited, a Tencent company. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in\n * compliance with the License. You may obtain a copy of the License at\n *\n * https://opensource.org/licenses/Apache-2.0",
    "results": [
        {
            "tool": "styler",
            "violations": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "checkstyle_idea",
            "violations": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/693/AdamUpdateFunc.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/checkstyle_idea/693/AdamUpdateFunc.java\nindex db580cf6828..34e072485b5 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/693/AdamUpdateFunc.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/checkstyle_idea/693/AdamUpdateFunc.java\n@@ -28,67 +28,69 @@ import org.apache.commons.logging.LogFactory;\n \n public class AdamUpdateFunc extends OptMMUpdateFunc {\n \n-  private static final Log LOG = LogFactory.getLog(AdamUpdateFunc.class);\n-\n-  public AdamUpdateFunc() {\n-    super();\n-  }\n-\n-  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr,\n-      double regParam, int iteration) {\n-    this(matId, factor, gamma, epsilon, beta, lr, regParam, iteration, 1);\n-  }\n-\n-  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr,\n-      double regParam, int iteration, int batchSize) {\n-    super(matId, new int[]{factor},\n-        new double[]{gamma, epsilon, beta, lr, regParam, iteration, batchSize});\n-  }\n-\n-  @Override\n-  public void update(RowBasedPartition partition, int factor, double[] scalars) {\n-    double gamma = scalars[0];\n-    double epsilon = scalars[1];\n-    double beta = scalars[2];\n-    double lr = scalars[3];\n-    double regParam = scalars[4];\n-    double epoch = scalars[5];\n-    double batchSize = scalars[6];\n-\n-    if (epoch == 0) {\n-      epoch = 1;\n+    private static final Log LOG = LogFactory.getLog(AdamUpdateFunc.class);\n+\n+    public AdamUpdateFunc() {\n+        super();\n     }\n \n-    double powBeta = Math.pow(beta, epoch);\n-    double powGamma = Math.pow(gamma, epoch);\n+    public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta,\n+                          double lr,\n+                          double regParam, int iteration) {\n+        this(matId, factor, gamma, epsilon, beta, lr, regParam, iteration, 1);\n+    }\n \n-    for (int f = 0; f < factor; f++) {\n-      ServerRow gradientServerRow = partition.getRow(f + 3 * factor);\n-      try {\n-        gradientServerRow.startWrite();\n-        Vector weight = ServerRowUtils.getVector(partition.getRow(f));\n-        Vector velocity = ServerRowUtils.getVector(partition.getRow(f + factor));\n-        Vector square = ServerRowUtils.getVector(partition.getRow(f + 2 * factor));\n-        Vector gradient = ServerRowUtils.getVector(gradientServerRow);\n+    public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta,\n+                          double lr,\n+                          double regParam, int iteration, int batchSize) {\n+        super(matId, new int[] {factor},\n+                new double[] {gamma, epsilon, beta, lr, regParam, iteration, batchSize});\n+    }\n \n-        if (batchSize > 1) {\n-          gradient.idiv(batchSize);\n+    @Override\n+    public void update(RowBasedPartition partition, int factor, double[] scalars) {\n+        double gamma = scalars[0];\n+        double epsilon = scalars[1];\n+        double beta = scalars[2];\n+        double lr = scalars[3];\n+        double regParam = scalars[4];\n+        double epoch = scalars[5];\n+        double batchSize = scalars[6];\n+\n+        if (epoch == 0) {\n+            epoch = 1;\n         }\n \n-        if (regParam != 0.0) {\n-          gradient.iaxpy(weight, regParam);\n+        double powBeta = Math.pow(beta, epoch);\n+        double powGamma = Math.pow(gamma, epoch);\n+\n+        for (int f = 0; f < factor; f++) {\n+            ServerRow gradientServerRow = partition.getRow(f + 3 * factor);\n+            try {\n+                gradientServerRow.startWrite();\n+                Vector weight = ServerRowUtils.getVector(partition.getRow(f));\n+                Vector velocity = ServerRowUtils.getVector(partition.getRow(f + factor));\n+                Vector square = ServerRowUtils.getVector(partition.getRow(f + 2 * factor));\n+                Vector gradient = ServerRowUtils.getVector(gradientServerRow);\n+\n+                if (batchSize > 1) {\n+                    gradient.idiv(batchSize);\n+                }\n+\n+                if (regParam != 0.0) {\n+                    gradient.iaxpy(weight, regParam);\n+                }\n+\n+                OptFuncs.iexpsmoothing(velocity, gradient, beta);\n+                OptFuncs.iexpsmoothing2(square, gradient, gamma);\n+\n+                Vector delta = OptFuncs.adamdelta(velocity, square, powBeta, powGamma);\n+                weight.iaxpy(delta, -lr);\n+                gradient.clear();\n+            } finally {\n+                gradientServerRow.endWrite();\n+            }\n         }\n-\n-        OptFuncs.iexpsmoothing(velocity, gradient, beta);\n-        OptFuncs.iexpsmoothing2(square, gradient, gamma);\n-\n-        Vector delta = OptFuncs.adamdelta(velocity, square, powBeta, powGamma);\n-        weight.iaxpy(delta, -lr);\n-        gradient.clear();\n-      } finally {\n-        gradientServerRow.endWrite();\n-      }\n     }\n-  }\n \n }\n",
            "diff_size": 94
        },
        {
            "tool": "naturalize",
            "violations": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "36",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 134).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "40",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 149).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "41",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 108).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/693/AdamUpdateFunc.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/naturalize/693/AdamUpdateFunc.java\nindex db580cf6828..fb6b61f6d55 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/693/AdamUpdateFunc.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/naturalize/693/AdamUpdateFunc.java\n@@ -15,7 +15,6 @@\n  *\n  */\n \n-\n package com.tencent.angel.ml.psf.optimizer;\n \n import com.tencent.angel.ml.math2.ufuncs.OptFuncs;\n@@ -34,15 +33,12 @@ public class AdamUpdateFunc extends OptMMUpdateFunc {\n     super();\n   }\n \n-  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr,\n-      double regParam, int iteration) {\n-    this(matId, factor, gamma, epsilon, beta, lr, regParam, iteration, 1);\n+  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr, double regParam, int iteration) {\n+  this(matId, factor, gamma, epsilon, beta, lr, regParam, iteration, 1);\n   }\n \n-  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr,\n-      double regParam, int iteration, int batchSize) {\n-    super(matId, new int[]{factor},\n-        new double[]{gamma, epsilon, beta, lr, regParam, iteration, batchSize});\n+  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr, double regParam, int iteration, int batchSize) {\n+    super(matId, new int[]{factor}, new double[]{gamma, epsilon, beta, lr, regParam, iteration, batchSize});\n   }\n \n   @Override\n@@ -91,4 +87,4 @@ public class AdamUpdateFunc extends OptMMUpdateFunc {\n     }\n   }\n \n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 9
        },
        {
            "tool": "codebuff",
            "violations": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "37",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 134).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "41",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 149).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                },
                {
                    "line": "42",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 108).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/693/AdamUpdateFunc.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/codebuff/693/AdamUpdateFunc.java\nindex db580cf6828..4b9cf02d9f5 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/693/AdamUpdateFunc.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/codebuff/693/AdamUpdateFunc.java\n@@ -34,19 +34,17 @@ public class AdamUpdateFunc extends OptMMUpdateFunc {\n     super();\n   }\n \n-  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr,\n-      double regParam, int iteration) {\n+  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr, double regParam, int iteration) {\n     this(matId, factor, gamma, epsilon, beta, lr, regParam, iteration, 1);\n   }\n \n-  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr,\n-      double regParam, int iteration, int batchSize) {\n-    super(matId, new int[]{factor},\n-        new double[]{gamma, epsilon, beta, lr, regParam, iteration, batchSize});\n+  public AdamUpdateFunc(int matId, int factor, double gamma, double epsilon, double beta, double lr, double regParam, int iteration, int batchSize) {\n+    super(matId, new int[]{factor}, new double[]{gamma, epsilon, beta, lr, regParam, iteration, batchSize});\n   }\n \n   @Override\n-  public void update(RowBasedPartition partition, int factor, double[] scalars) {\n+  public void update(\n+    RowBasedPartition partition, int factor, double[] scalars) {\n     double gamma = scalars[0];\n     double epsilon = scalars[1];\n     double beta = scalars[2];\n@@ -54,14 +52,12 @@ public class AdamUpdateFunc extends OptMMUpdateFunc {\n     double regParam = scalars[4];\n     double epoch = scalars[5];\n     double batchSize = scalars[6];\n-\n     if (epoch == 0) {\n       epoch = 1;\n     }\n \n     double powBeta = Math.pow(beta, epoch);\n     double powGamma = Math.pow(gamma, epoch);\n-\n     for (int f = 0; f < factor; f++) {\n       ServerRow gradientServerRow = partition.getRow(f + 3 * factor);\n       try {\n@@ -70,7 +66,6 @@ public class AdamUpdateFunc extends OptMMUpdateFunc {\n         Vector velocity = ServerRowUtils.getVector(partition.getRow(f + factor));\n         Vector square = ServerRowUtils.getVector(partition.getRow(f + 2 * factor));\n         Vector gradient = ServerRowUtils.getVector(gradientServerRow);\n-\n         if (batchSize > 1) {\n           gradient.idiv(batchSize);\n         }\n@@ -78,10 +73,8 @@ public class AdamUpdateFunc extends OptMMUpdateFunc {\n         if (regParam != 0.0) {\n           gradient.iaxpy(weight, regParam);\n         }\n-\n         OptFuncs.iexpsmoothing(velocity, gradient, beta);\n         OptFuncs.iexpsmoothing2(square, gradient, gamma);\n-\n         Vector delta = OptFuncs.adamdelta(velocity, square, powBeta, powGamma);\n         weight.iaxpy(delta, -lr);\n         gradient.clear();\n@@ -91,4 +84,4 @@ public class AdamUpdateFunc extends OptMMUpdateFunc {\n     }\n   }\n \n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 14
        },
        {
            "tool": "styler_random",
            "violations": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "violations": [
                {
                    "line": "6",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "checkstyle_idea",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}