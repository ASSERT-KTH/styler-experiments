{
    "project_name": "Angel-ML-angel",
    "violation_id": "1058",
    "information": {
        "violations": [
            {
                "line": "54",
                "severity": "error",
                "message": "No trailing whitespace allowed.",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
            }
        ]
    },
    "source_code": "  /**\n   * get gradient over each of predictions, given existing information.\n   * \n   * @param preds: prediction of current round\n   * @param dataMeta information about labels, weights, groups in rank\n   * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient",
    "results": [
        {
            "tool": "styler",
            "violations": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "checkstyle_idea",
            "violations": [
                {
                    "line": "58",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1058/SoftmaxMultiClassObj.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/checkstyle_idea/1058/SoftmaxMultiClassObj.java\nindex f05e5439160..378a37148c1 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1058/SoftmaxMultiClassObj.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/checkstyle_idea/1058/SoftmaxMultiClassObj.java\n@@ -14,6 +14,7 @@\n  * the License.\n  *\n  */\n+\n package com.tencent.angel.ml.objective;\n \n import com.tencent.angel.ml.RegTree.DataMeta;\n@@ -33,127 +34,127 @@ import java.util.List;\n \n public class SoftmaxMultiClassObj implements ObjFunc {\n \n-  private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n-\n-  public RegTTrainParam param;\n-  public int numClass;\n-  private boolean outputProb;\n-\n-  public SoftmaxMultiClassObj(RegTTrainParam param, boolean outputProb) {\n-    this.param = param;\n-    this.numClass = param.numClass;\n-    this.outputProb = outputProb;\n-  }\n-\n-  public SoftmaxMultiClassObj(RegTTrainParam param) {\n-    this(param, true);\n-  }\n-\n-  /**\n-   * get gradient over each of predictions, given existing information.\n-   * \n-   * @param preds: prediction of current round\n-   * @param dataMeta information about labels, weights, groups in rank\n-   * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n-   *        and second order gradient in\n-   */\n-  @Override\n-  public List<GradPair> getGradient(float[] preds, DataMeta dataMeta, int iteration) {\n-    assert preds.length == this.numClass * dataMeta.labels.length;\n-    List<GradPair> rec = new ArrayList<GradPair>();\n-    int ndata = preds.length / numClass;\n-    int labelError = -1;\n-    float[] tmp = new float[numClass];\n-    for (int insIdx = 0; insIdx < ndata; insIdx++) {\n-      for (int k = 0; k < numClass; k++) {\n-        tmp[k] = preds[insIdx * numClass + k];\n-      }\n-      MathUtils.softmax(tmp);\n-      int label = (int) dataMeta.labels[insIdx];\n-      if (label < 0 || label >= numClass) {\n-        labelError = label;\n-        label = 0;\n-      }\n-      float wt = dataMeta.getWeight(insIdx);\n-      for (int k = 0; k < numClass; ++k) {\n-        float p = tmp[k];\n-        float h = 2.0f * p * (1.0f - p) * wt;\n-        if (label == k) {\n-          GradPair pair = new GradPair((p - 1.0f) * wt, h);\n-          rec.add(pair);\n-        } else {\n-          GradPair pair = new GradPair(p * wt, h);\n-          rec.add(pair);\n-        }\n-      }\n+    private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n+\n+    public RegTTrainParam param;\n+    public int numClass;\n+    private boolean outputProb;\n+\n+    public SoftmaxMultiClassObj(RegTTrainParam param, boolean outputProb) {\n+        this.param = param;\n+        this.numClass = param.numClass;\n+        this.outputProb = outputProb;\n     }\n-    if (labelError >= 0 && labelError < numClass) {\n-      LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n-          + \"numClass = %d, but found %d in label\", numClass, labelError));\n+\n+    public SoftmaxMultiClassObj(RegTTrainParam param) {\n+        this(param, true);\n     }\n-    return rec;\n-  }\n-\n-  public List<Float> transform(List<Float> preds, boolean prob) {\n-    List<Float> rec = new ArrayList<Float>();\n-    int ndata = preds.size() / numClass;\n-    float[] tmp = new float[numClass];\n-\n-    for (int insIdx = 0; insIdx < ndata; insIdx++) {\n-      for (int k = 0; k < numClass; k++) {\n-        tmp[k] = preds.get(insIdx * numClass + k);\n-      }\n-      if (!prob) {\n-        rec.add((float) MathUtils.findMaxIndex(tmp));\n-      } else {\n-        MathUtils.softmax(tmp);\n-        for (int k = 0; k < numClass; k++) {\n-          preds.set(insIdx * numClass + k, tmp[k]);\n+\n+    /**\n+     * get gradient over each of predictions, given existing information.\n+     *\n+     * @param preds:    prediction of current round\n+     * @param dataMeta  information about labels, weights, groups in rank\n+     * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n+     *                  and second order gradient in\n+     */\n+    @Override\n+    public List<GradPair> getGradient(float[] preds, DataMeta dataMeta, int iteration) {\n+        assert preds.length == this.numClass * dataMeta.labels.length;\n+        List<GradPair> rec = new ArrayList<GradPair>();\n+        int ndata = preds.length / numClass;\n+        int labelError = -1;\n+        float[] tmp = new float[numClass];\n+        for (int insIdx = 0; insIdx < ndata; insIdx++) {\n+            for (int k = 0; k < numClass; k++) {\n+                tmp[k] = preds[insIdx * numClass + k];\n+            }\n+            MathUtils.softmax(tmp);\n+            int label = (int) dataMeta.labels[insIdx];\n+            if (label < 0 || label >= numClass) {\n+                labelError = label;\n+                label = 0;\n+            }\n+            float wt = dataMeta.getWeight(insIdx);\n+            for (int k = 0; k < numClass; ++k) {\n+                float p = tmp[k];\n+                float h = 2.0f * p * (1.0f - p) * wt;\n+                if (label == k) {\n+                    GradPair pair = new GradPair((p - 1.0f) * wt, h);\n+                    rec.add(pair);\n+                } else {\n+                    GradPair pair = new GradPair(p * wt, h);\n+                    rec.add(pair);\n+                }\n+            }\n+        }\n+        if (labelError >= 0 && labelError < numClass) {\n+            LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n+                    + \"numClass = %d, but found %d in label\", numClass, labelError));\n         }\n-      }\n+        return rec;\n+    }\n+\n+    public List<Float> transform(List<Float> preds, boolean prob) {\n+        List<Float> rec = new ArrayList<Float>();\n+        int ndata = preds.size() / numClass;\n+        float[] tmp = new float[numClass];\n+\n+        for (int insIdx = 0; insIdx < ndata; insIdx++) {\n+            for (int k = 0; k < numClass; k++) {\n+                tmp[k] = preds.get(insIdx * numClass + k);\n+            }\n+            if (!prob) {\n+                rec.add((float) MathUtils.findMaxIndex(tmp));\n+            } else {\n+                MathUtils.softmax(tmp);\n+                for (int k = 0; k < numClass; k++) {\n+                    preds.set(insIdx * numClass + k, tmp[k]);\n+                }\n+            }\n+        }\n+        // if (!prob) {\n+        // preds.clear();\n+        // preds.addAll(rec);\n+        // }\n+        return rec;\n+    }\n+\n+    @Override\n+    public String defaultEvalMetric() {\n+        return \"merror\";\n+    }\n+\n+    /**\n+     * transform prediction values, this is only called when Prediction is called preds: prediction\n+     * values, saves to this vector as well\n+     *\n+     * @param preds\n+     */\n+    @Override\n+    public void predTransform(List<Float> preds) {\n+        this.transform(preds, this.outputProb);\n+    }\n+\n+    /**\n+     * transform prediction values, this is only called when Eval is called usually it redirect to\n+     * predTransform preds: prediction values, saves to this vector as well\n+     *\n+     * @param preds\n+     */\n+    @Override\n+    public void evalTransform(List<Float> preds) {\n+        this.transform(preds, true);\n+    }\n+\n+    /**\n+     * transform probability value back to margin this is used to transform user-set base_score back\n+     * to margin used by gradient boosting\n+     *\n+     * @param base_score\n+     */\n+    @Override\n+    public float prob2Margin(float base_score) {\n+        return 0;\n     }\n-    // if (!prob) {\n-    // preds.clear();\n-    // preds.addAll(rec);\n-    // }\n-    return rec;\n-  }\n-\n-  @Override\n-  public String defaultEvalMetric() {\n-    return \"merror\";\n-  }\n-\n-  /**\n-   * transform prediction values, this is only called when Prediction is called preds: prediction\n-   * values, saves to this vector as well\n-   *\n-   * @param preds\n-   */\n-  @Override\n-  public void predTransform(List<Float> preds) {\n-    this.transform(preds, this.outputProb);\n-  }\n-\n-  /**\n-   * transform prediction values, this is only called when Eval is called usually it redirect to\n-   * predTransform preds: prediction values, saves to this vector as well\n-   *\n-   * @param preds\n-   */\n-  @Override\n-  public void evalTransform(List<Float> preds) {\n-    this.transform(preds, true);\n-  }\n-\n-  /**\n-   * transform probability value back to margin this is used to transform user-set base_score back\n-   * to margin used by gradient boosting\n-   *\n-   * @param base_score\n-   */\n-  @Override\n-  public float prob2Margin(float base_score) {\n-    return 0;\n-  }\n }\n",
            "diff_size": 207
        },
        {
            "tool": "naturalize",
            "violations": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1058/SoftmaxMultiClassObj.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/naturalize/1058/SoftmaxMultiClassObj.java\nindex f05e5439160..841afaa7082 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1058/SoftmaxMultiClassObj.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/naturalize/1058/SoftmaxMultiClassObj.java\n@@ -90,7 +90,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n     if (labelError >= 0 && labelError < numClass) {\n       LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n           + \"numClass = %d, but found %d in label\", numClass, labelError));\n-    }\n+  }\n     return rec;\n   }\n \n@@ -156,4 +156,4 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n   public float prob2Margin(float base_score) {\n     return 0;\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 2
        },
        {
            "tool": "codebuff",
            "violations": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                },
                {
                    "line": "94",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 153).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1058/SoftmaxMultiClassObj.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/codebuff/1058/SoftmaxMultiClassObj.java\nindex f05e5439160..4297d4bbe80 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1058/SoftmaxMultiClassObj.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/codebuff/1058/SoftmaxMultiClassObj.java\n@@ -22,7 +22,6 @@ import com.tencent.angel.ml.param.RegTTrainParam;\n import com.tencent.angel.ml.utils.MathUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-\n import java.util.ArrayList;\n import java.util.List;\n \n@@ -36,6 +35,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n   private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n \n   public RegTTrainParam param;\n+\n   public int numClass;\n   private boolean outputProb;\n \n@@ -57,6 +57,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n    *        and second order gradient in\n    */\n+\n   @Override\n   public List<GradPair> getGradient(float[] preds, DataMeta dataMeta, int iteration) {\n     assert preds.length == this.numClass * dataMeta.labels.length;\n@@ -69,6 +70,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n         tmp[k] = preds[insIdx * numClass + k];\n       }\n       MathUtils.softmax(tmp);\n+\n       int label = (int) dataMeta.labels[insIdx];\n       if (label < 0 || label >= numClass) {\n         labelError = label;\n@@ -87,9 +89,9 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n         }\n       }\n     }\n+\n     if (labelError >= 0 && labelError < numClass) {\n-      LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n-          + \"numClass = %d, but found %d in label\", numClass, labelError));\n+      LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \" + \"numClass = %d, but found %d in label\", numClass, labelError));\n     }\n     return rec;\n   }\n@@ -98,11 +100,11 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n     List<Float> rec = new ArrayList<Float>();\n     int ndata = preds.size() / numClass;\n     float[] tmp = new float[numClass];\n-\n     for (int insIdx = 0; insIdx < ndata; insIdx++) {\n       for (int k = 0; k < numClass; k++) {\n         tmp[k] = preds.get(insIdx * numClass + k);\n       }\n+\n       if (!prob) {\n         rec.add((float) MathUtils.findMaxIndex(tmp));\n       } else {\n@@ -130,6 +132,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param preds\n    */\n+\n   @Override\n   public void predTransform(List<Float> preds) {\n     this.transform(preds, this.outputProb);\n@@ -141,6 +144,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param preds\n    */\n+\n   @Override\n   public void evalTransform(List<Float> preds) {\n     this.transform(preds, true);\n@@ -152,8 +156,9 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param base_score\n    */\n+\n   @Override\n   public float prob2Margin(float base_score) {\n     return 0;\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 13
        },
        {
            "tool": "styler_random",
            "violations": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "violations": [
                {
                    "line": "53",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1058/SoftmaxMultiClassObj.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/styler/05_predictions/three_grams/files-repaired/1058/SoftmaxMultiClassObj.java\nindex f05e5439160..8bd384b4245 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1058/SoftmaxMultiClassObj.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/styler/05_predictions/three_grams/files-repaired/1058/SoftmaxMultiClassObj.java\n@@ -48,7 +48,6 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n   public SoftmaxMultiClassObj(RegTTrainParam param) {\n     this(param, true);\n   }\n-\n   /**\n    * get gradient over each of predictions, given existing information.\n    * \n",
            "diff_size": 1
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "checkstyle_idea",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}