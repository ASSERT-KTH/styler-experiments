{
    "project_name": "Angel-ML-angel",
    "violation_id": "1138",
    "information": {
        "violations": [
            {
                "line": "54",
                "severity": "error",
                "message": "No trailing whitespace allowed.",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
            }
        ]
    },
    "source_code": "  /**\n   * get gradient over each of predictions, given existing information.\n   * \n   * @param preds: prediction of current round\n   * @param dataStore information about labels, weights, groups in rank\n   * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient",
    "results": [
        {
            "tool": "styler",
            "violations": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "checkstyle_idea",
            "violations": [
                {
                    "line": "58",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 102).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1138/SoftmaxMultiClassObj.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/checkstyle_idea/1138/SoftmaxMultiClassObj.java\nindex c927aa8a4ba..d055de3e7a3 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1138/SoftmaxMultiClassObj.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/checkstyle_idea/1138/SoftmaxMultiClassObj.java\n@@ -14,6 +14,7 @@\n  * the License.\n  *\n  */\n+\n package com.tencent.angel.ml.objective;\n \n import com.tencent.angel.ml.RegTree.GradPair;\n@@ -33,121 +34,121 @@ import java.util.List;\n \n public class SoftmaxMultiClassObj implements ObjFunc {\n \n-  private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n-\n-  public RegTParam param;\n-  public int numClass;\n-  private boolean outputProb;\n-\n-  public SoftmaxMultiClassObj(RegTParam param, boolean outputProb) {\n-    this.param = param;\n-    this.numClass = param.numClass;\n-    this.outputProb = outputProb;\n-  }\n-\n-  public SoftmaxMultiClassObj(RegTParam param) {\n-    this(param, true);\n-  }\n-\n-  /**\n-   * get gradient over each of predictions, given existing information.\n-   * \n-   * @param preds: prediction of current round\n-   * @param dataStore information about labels, weights, groups in rank\n-   * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n-   *        and second order gradient in\n-   */\n-  @Override\n-  public List<GradPair> calGrad(float[] preds, RegTDataStore dataStore, int iteration) {\n-    assert preds.length == this.numClass * dataStore.labels.length;\n-    List<GradPair> rec = new ArrayList<GradPair>();\n-    int ndata = preds.length / numClass;\n-    int labelError = -1;\n-    float[] tmp = new float[numClass];\n-    for (int insIdx = 0; insIdx < ndata; insIdx++) {\n-      System.arraycopy(preds, insIdx * numClass, tmp, 0, numClass);\n-      MathUtils.softmax(tmp);\n-      int label = (int) dataStore.labels[insIdx];\n-      if (label < 0 || label >= numClass) {\n-        labelError = label;\n-        label = 0;\n-      }\n-      float wt = dataStore.getWeight(insIdx);\n-      for (int k = 0; k < numClass; ++k) {\n-        float p = tmp[k];\n-        float h = 2.0f * p * (1.0f - p) * wt;\n-        if (label == k) {\n-          GradPair pair = new GradPair((p - 1.0f) * wt, h);\n-          rec.add(pair);\n-        } else {\n-          GradPair pair = new GradPair(p * wt, h);\n-          rec.add(pair);\n-        }\n-      }\n+    private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n+\n+    public RegTParam param;\n+    public int numClass;\n+    private boolean outputProb;\n+\n+    public SoftmaxMultiClassObj(RegTParam param, boolean outputProb) {\n+        this.param = param;\n+        this.numClass = param.numClass;\n+        this.outputProb = outputProb;\n     }\n-    if (labelError >= 0 && labelError < numClass) {\n-      LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n-          + \"numClass = %d, but found %d in label\", numClass, labelError));\n+\n+    public SoftmaxMultiClassObj(RegTParam param) {\n+        this(param, true);\n     }\n-    return rec;\n-  }\n-\n-  public List<Float> transform(List<Float> preds, boolean prob) {\n-    List<Float> rec = new ArrayList<Float>();\n-    int ndata = preds.size() / numClass;\n-    float[] tmp = new float[numClass];\n-\n-    for (int insIdx = 0; insIdx < ndata; insIdx++) {\n-      for (int k = 0; k < numClass; k++) {\n-        tmp[k] = preds.get(insIdx * numClass + k);\n-      }\n-      if (!prob) {\n-        rec.add((float) MathUtils.findMaxIndex(tmp));\n-      } else {\n-        MathUtils.softmax(tmp);\n-        for (int k = 0; k < numClass; k++) {\n-          preds.set(insIdx * numClass + k, tmp[k]);\n+\n+    /**\n+     * get gradient over each of predictions, given existing information.\n+     *\n+     * @param preds:    prediction of current round\n+     * @param dataStore information about labels, weights, groups in rank\n+     * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n+     *                  and second order gradient in\n+     */\n+    @Override\n+    public List<GradPair> calGrad(float[] preds, RegTDataStore dataStore, int iteration) {\n+        assert preds.length == this.numClass * dataStore.labels.length;\n+        List<GradPair> rec = new ArrayList<GradPair>();\n+        int ndata = preds.length / numClass;\n+        int labelError = -1;\n+        float[] tmp = new float[numClass];\n+        for (int insIdx = 0; insIdx < ndata; insIdx++) {\n+            System.arraycopy(preds, insIdx * numClass, tmp, 0, numClass);\n+            MathUtils.softmax(tmp);\n+            int label = (int) dataStore.labels[insIdx];\n+            if (label < 0 || label >= numClass) {\n+                labelError = label;\n+                label = 0;\n+            }\n+            float wt = dataStore.getWeight(insIdx);\n+            for (int k = 0; k < numClass; ++k) {\n+                float p = tmp[k];\n+                float h = 2.0f * p * (1.0f - p) * wt;\n+                if (label == k) {\n+                    GradPair pair = new GradPair((p - 1.0f) * wt, h);\n+                    rec.add(pair);\n+                } else {\n+                    GradPair pair = new GradPair(p * wt, h);\n+                    rec.add(pair);\n+                }\n+            }\n+        }\n+        if (labelError >= 0 && labelError < numClass) {\n+            LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n+                    + \"numClass = %d, but found %d in label\", numClass, labelError));\n         }\n-      }\n+        return rec;\n+    }\n+\n+    public List<Float> transform(List<Float> preds, boolean prob) {\n+        List<Float> rec = new ArrayList<Float>();\n+        int ndata = preds.size() / numClass;\n+        float[] tmp = new float[numClass];\n+\n+        for (int insIdx = 0; insIdx < ndata; insIdx++) {\n+            for (int k = 0; k < numClass; k++) {\n+                tmp[k] = preds.get(insIdx * numClass + k);\n+            }\n+            if (!prob) {\n+                rec.add((float) MathUtils.findMaxIndex(tmp));\n+            } else {\n+                MathUtils.softmax(tmp);\n+                for (int k = 0; k < numClass; k++) {\n+                    preds.set(insIdx * numClass + k, tmp[k]);\n+                }\n+            }\n+        }\n+        return rec;\n+    }\n+\n+    @Override\n+    public String defaultEvalMetric() {\n+        return \"merror\";\n+    }\n+\n+    /**\n+     * transform prediction values, this is only called when Prediction is called preds: prediction\n+     * values, saves to this vector as well\n+     *\n+     * @param preds\n+     */\n+    @Override\n+    public void transPred(List<Float> preds) {\n+        this.transform(preds, this.outputProb);\n+    }\n+\n+    /**\n+     * transform prediction values, this is only called when Eval is called usually it redirect to\n+     * transPred preds: prediction values, saves to this vector as well\n+     *\n+     * @param preds\n+     */\n+    @Override\n+    public void transEval(List<Float> preds) {\n+        this.transform(preds, true);\n+    }\n+\n+    /**\n+     * transform probability value back to margin this is used to transform user-set base_score back\n+     * to margin used by gradient boosting\n+     *\n+     * @param base_score\n+     */\n+    @Override\n+    public float prob2Margin(float base_score) {\n+        return 0;\n     }\n-    return rec;\n-  }\n-\n-  @Override\n-  public String defaultEvalMetric() {\n-    return \"merror\";\n-  }\n-\n-  /**\n-   * transform prediction values, this is only called when Prediction is called preds: prediction\n-   * values, saves to this vector as well\n-   *\n-   * @param preds\n-   */\n-  @Override\n-  public void transPred(List<Float> preds) {\n-    this.transform(preds, this.outputProb);\n-  }\n-\n-  /**\n-   * transform prediction values, this is only called when Eval is called usually it redirect to\n-   * transPred preds: prediction values, saves to this vector as well\n-   *\n-   * @param preds\n-   */\n-  @Override\n-  public void transEval(List<Float> preds) {\n-    this.transform(preds, true);\n-  }\n-\n-  /**\n-   * transform probability value back to margin this is used to transform user-set base_score back\n-   * to margin used by gradient boosting\n-   *\n-   * @param base_score\n-   */\n-  @Override\n-  public float prob2Margin(float base_score) {\n-    return 0;\n-  }\n }\n",
            "diff_size": 195
        },
        {
            "tool": "naturalize",
            "violations": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1138/SoftmaxMultiClassObj.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/naturalize/1138/SoftmaxMultiClassObj.java\nindex c927aa8a4ba..c29fcb95b57 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1138/SoftmaxMultiClassObj.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/naturalize/1138/SoftmaxMultiClassObj.java\n@@ -88,7 +88,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n     if (labelError >= 0 && labelError < numClass) {\n       LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n           + \"numClass = %d, but found %d in label\", numClass, labelError));\n-    }\n+  }\n     return rec;\n   }\n \n@@ -150,4 +150,4 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n   public float prob2Margin(float base_score) {\n     return 0;\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 2
        },
        {
            "tool": "codebuff",
            "violations": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                },
                {
                    "line": "92",
                    "severity": "error",
                    "message": "Line is longer than 100 characters (found 153).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.sizes.LineLengthCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1138/SoftmaxMultiClassObj.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/codebuff/1138/SoftmaxMultiClassObj.java\nindex c927aa8a4ba..b7e89882cec 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1138/SoftmaxMultiClassObj.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/codebuff/1138/SoftmaxMultiClassObj.java\n@@ -22,7 +22,6 @@ import com.tencent.angel.ml.param.RegTParam;\n import com.tencent.angel.ml.utils.MathUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-\n import java.util.ArrayList;\n import java.util.List;\n \n@@ -36,6 +35,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n   private static final Log LOG = LogFactory.getLog(SoftmaxMultiClassObj.class);\n \n   public RegTParam param;\n+\n   public int numClass;\n   private boolean outputProb;\n \n@@ -57,6 +57,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    * @param iteration current iteration number. return:_gpair output of get gradient, saves gradient\n    *        and second order gradient in\n    */\n+\n   @Override\n   public List<GradPair> calGrad(float[] preds, RegTDataStore dataStore, int iteration) {\n     assert preds.length == this.numClass * dataStore.labels.length;\n@@ -67,6 +68,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n     for (int insIdx = 0; insIdx < ndata; insIdx++) {\n       System.arraycopy(preds, insIdx * numClass, tmp, 0, numClass);\n       MathUtils.softmax(tmp);\n+\n       int label = (int) dataStore.labels[insIdx];\n       if (label < 0 || label >= numClass) {\n         labelError = label;\n@@ -85,9 +87,9 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n         }\n       }\n     }\n+\n     if (labelError >= 0 && labelError < numClass) {\n-      LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \"\n-          + \"numClass = %d, but found %d in label\", numClass, labelError));\n+      LOG.error(String.format(\"SoftmaxMultiClassObj: label must be in [0, num_class), \" + \"numClass = %d, but found %d in label\", numClass, labelError));\n     }\n     return rec;\n   }\n@@ -96,11 +98,11 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n     List<Float> rec = new ArrayList<Float>();\n     int ndata = preds.size() / numClass;\n     float[] tmp = new float[numClass];\n-\n     for (int insIdx = 0; insIdx < ndata; insIdx++) {\n       for (int k = 0; k < numClass; k++) {\n         tmp[k] = preds.get(insIdx * numClass + k);\n       }\n+\n       if (!prob) {\n         rec.add((float) MathUtils.findMaxIndex(tmp));\n       } else {\n@@ -124,6 +126,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param preds\n    */\n+\n   @Override\n   public void transPred(List<Float> preds) {\n     this.transform(preds, this.outputProb);\n@@ -135,6 +138,7 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param preds\n    */\n+\n   @Override\n   public void transEval(List<Float> preds) {\n     this.transform(preds, true);\n@@ -146,8 +150,9 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n    *\n    * @param base_score\n    */\n+\n   @Override\n   public float prob2Margin(float base_score) {\n     return 0;\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 13
        },
        {
            "tool": "styler_random",
            "violations": [
                {
                    "line": "54",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "violations": [
                {
                    "line": "53",
                    "severity": "error",
                    "message": "No trailing whitespace allowed.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1138/SoftmaxMultiClassObj.java b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/styler/05_predictions/three_grams/files-repaired/1138/SoftmaxMultiClassObj.java\nindex c927aa8a4ba..3fefb959e23 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/violations/1138/SoftmaxMultiClassObj.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/src/experiments/projects/Angel-ML-angel/styler/05_predictions/three_grams/files-repaired/1138/SoftmaxMultiClassObj.java\n@@ -48,7 +48,6 @@ public class SoftmaxMultiClassObj implements ObjFunc {\n   public SoftmaxMultiClassObj(RegTParam param) {\n     this(param, true);\n   }\n-\n   /**\n    * get gradient over each of predictions, given existing information.\n    * \n",
            "diff_size": 1
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "checkstyle_idea",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}